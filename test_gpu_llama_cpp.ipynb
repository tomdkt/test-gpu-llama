{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNlB+5OVmSiG0ynQ/grsyE7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomdkt/test-gpu-llama/blob/main/test_gpu_llama_cpp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /etc/os-release\n",
        "!nvcc --version\n",
        "import torch\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "7XOrFXX8ROQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# compile cuda llama-cpp-python. ~25 min\n",
        "# !pip cache purge  # or pip cache remove llama-cpp-python\n",
        "# !pip uninstall llama-cpp-python\n",
        "# !CMAKE_ARGS=\"-DGGML_CUDA=on\" FORCE_CMAKE=1 pip install llama-cpp-python\n",
        "\n",
        "# download llama driver\n",
        "# import os\n",
        "# import llama_cpp\n",
        "\n",
        "# lib_path = os.path.dirname(llama_cpp.__file__)\n",
        "# zip_path = \"/content/llama_cpp.zip\"\n",
        "# print(f\"Zipping from: {lib_path}\")\n",
        "# !zip -r /content/llama_cpp.zip \"$lib_path\"\n",
        "\n",
        "# # 3. (Opcional) download zip(~26mb)\n",
        "# from google.colab import files\n",
        "# files.download(zip_path)\n",
        "\n",
        "from llama_cpp import llama_cpp\n",
        "# gpu enable must be True\n",
        "print(llama_cpp.llama_supports_gpu_offload())"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "LSzdZM6ILO8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upload llama cpp driver\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # select `llama_cpp.zip`\n",
        "!unzip -o llama_cpp.zip '*/llama_cpp/*' -d temp_llama\n",
        "!cp -r temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp /usr/local/lib/python3.11/dist-packages/\n",
        "\n",
        "\n",
        "!ls /usr/local/lib/python3.11/dist-packages/llama_cpp\n",
        "!pip install diskcache\n",
        "import sys\n",
        "sys.path.append(\"/usr/local/lib/python3.11/dist-packages\")\n",
        "\n",
        "from llama_cpp import llama_cpp\n",
        "# gpu enable must be True\n",
        "print(llama_cpp.llama_supports_gpu_offload())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UhdMD2znsg09",
        "outputId": "eb6fb6c8-f996-47e5-de78-012d3592166a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  llama_cpp.zip\n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/_logger.py  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/llama_grammar.py  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/_ggml.py  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/__pycache__/_logger.cpython-311.pyc  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/__pycache__/_ctypes_extensions.cpython-311.pyc  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/__pycache__/_ggml.cpython-311.pyc  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/__pycache__/llama.cpython-311.pyc  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/__pycache__/__init__.cpython-311.pyc  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/__pycache__/llama_chat_format.cpython-311.pyc  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/__pycache__/llava_cpp.cpython-311.pyc  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/__pycache__/llama_cache.cpython-311.pyc  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/__pycache__/llama_grammar.cpython-311.pyc  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/__pycache__/llama_speculative.cpython-311.pyc  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/__pycache__/_utils.cpython-311.pyc  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/__pycache__/llama_cpp.cpython-311.pyc  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/__pycache__/_internals.cpython-311.pyc  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/__pycache__/llama_types.cpython-311.pyc  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/__pycache__/llama_tokenizer.cpython-311.pyc  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/_utils.py  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/llama_speculative.py  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/llama_types.py  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/llama_chat_format.py  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/llama_cache.py  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/llama.py  \n",
            " extracting: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/py.typed  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/llama_cpp.py  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/_ctypes_extensions.py  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/llava_cpp.py  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/_internals.py  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/llama_tokenizer.py  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/server/app.py  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/server/errors.py  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/server/__pycache__/errors.cpython-311.pyc  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/server/__pycache__/__init__.cpython-311.pyc  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/server/__pycache__/cli.cpython-311.pyc  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/server/__pycache__/types.cpython-311.pyc  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/server/__pycache__/__main__.cpython-311.pyc  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/server/__pycache__/app.cpython-311.pyc  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/server/__pycache__/settings.cpython-311.pyc  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/server/__pycache__/model.cpython-311.pyc  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/server/types.py  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/server/model.py  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/server/settings.py  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/server/cli.py  \n",
            " extracting: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/server/__init__.py  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/server/__main__.py  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/__init__.py  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/lib/libllava.so  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/lib/libggml.so  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/lib/libggml-cpu.so  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/lib/libllama.so  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/lib/libggml-base.so  \n",
            "  inflating: temp_llama/usr/local/lib/python3.11/dist-packages/llama_cpp/lib/libggml-cuda.so  \n",
            "_ctypes_extensions.py  llama_cache.py\t     llama_speculative.py  __pycache__\n",
            "_ggml.py\t       llama_chat_format.py  llama_tokenizer.py    py.typed\n",
            "__init__.py\t       llama_cpp.py\t     llama_types.py\t   server\n",
            "_internals.py\t       llama_grammar.py      llava_cpp.py\t   _utils.py\n",
            "lib\t\t       llama.py\t\t     _logger.py\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.11/dist-packages (5.6.3)\n",
            "/usr/local/lib/python3.11/dist-packages/llama_cpp/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama.from_pretrained(\n",
        "\trepo_id=\"google/gemma-3-4b-it-qat-q4_0-gguf\",\n",
        "\tfilename=\"gemma-3-4b-it-q4_0.gguf\",\n",
        "  n_gpu_layers=-1\n",
        ")"
      ],
      "metadata": {
        "id": "derLyXmEAEGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "response = llm.create_chat_completion(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": \"oi!!! como vc est√°?\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ],\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "# Accumulate the full response\n",
        "full_response = \"\"\n",
        "\n",
        "# Display initial HTML structure for dynamic updates\n",
        "display(HTML(\"<b>Resposta:</b> <div id='response'></div><br><br><br>\"))\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n",
        "    #full_response += content\n",
        "\n",
        "    # Update the HTML content with the latest response chunk\n",
        "    # Using `.format()` instead of f-strings to avoid backslash issues\n",
        "    display(HTML(\"<script>document.getElementById('response').innerHTML += '{}';</script>\".format(content.replace('`', '\\\\`'))), display_id='response_update')\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "lgsT0xavs7sM",
        "outputId": "fcc4b5ca-b6bd-4883-91aa-b6efa2feac81"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<b>Resposta:</b> <div id='response'></div><br><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script>document.getElementById('response').innerHTML += '';</script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =     328.89 ms\n",
            "llama_perf_context_print: prompt eval time =     328.52 ms /    16 tokens (   20.53 ms per token,    48.70 tokens per second)\n",
            "llama_perf_context_print:        eval time =     447.75 ms /    20 runs   (   22.39 ms per token,    44.67 tokens per second)\n",
            "llama_perf_context_print:       total time =     857.31 ms /    36 tokens\n"
          ]
        }
      ]
    }
  ]
}